# %%
# !python --version

# %%

# # Install Pytorch & other libraries, make sure to match your GPU driver version
# %pip install "torch==2.5.1" tensorboard "setuptools<71.0.0"  --index-url https://download.pytorch.org/whl/cu121

# # Install flash-attn
# %pip install flash-attn 

# # Install Hugging Face libraries
# %pip install  --upgrade \
#   "transformers==4.48.1" \
#   "datasets==3.1.0" \
#   "accelerate==1.3.0" \
#   "hf-transfer==0.1.9" \
#   "deepspeed==0.15.4" \
#   "trl==0.14.0"\
#   "datasets"\
#   "jinja2>=3.1.0"\
#   "trl"

# # install vLLM 
# %pip install "vllm==0.7.0"

# ## IMPORTANT: If you want to run the notebook and the interactive cells you also need to install the following libraries:
# # But first read it the blog post and then decide as they might conflict with the libraries for distributed training. 
# %pip install "peft==0.14.0" "bitsandbytes==0.45.0"

import os
from huggingface_hub import login
from accelerate import Accelerator
from transformers import AutoTokenizer, DataCollatorWithPadding,TrainerCallback, AutoModelForCausalLM, AutoConfig, TrainingArguments, Trainer
from datasets import load_dataset
import jinja2
import re
import ast
from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig
import torch
import wandb
from tqdm import tqdm
import time



import os, json, inspect

import subprocess



from subprocess import Popen, run

import requests

response = requests.post(
    "http://localhost:8000/generate/",
    json={"prompts": ["Hello, world!"], "max_tokens": 32}
)
print("WORKS1",response.json())



tokens = ["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","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","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","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"] 
for i in tokens:
    try:
        login(token=i, add_to_git_credential=False) # ADD YOUR TOKEN HERE
        print(f"Logged in with token: {i}")
    except Exception as e:
        print(f"Failed to log in with token {i}: {e}")
        continue
    break


ATTACK = True
accelerator = Accelerator()
# Load dataset from Hugging Face Hub
dataset_id = "Jiayi-Pan/Countdown-Tasks-3to4"
dataset = load_dataset(dataset_id, split="train")
# select a random subset of 50k samples
dataset = dataset.shuffle(seed=42).select(range(50000))

# Load tokenizer from Hugging Face Hub to format the dataset to our "r1" prompt 
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct",
                                          trust_remote_code=True,
                                          use_fast=False,
                                          padding_side="left",          # ‚Üê crucial
                                          )
def generate_r1_prompt(numbers, target):
  r1_prefix = [{
      "role": "system",
      "content": "You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer."
    },
    { 
      "role": "user",
      "content": f"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>."
    },
    {
      "role": "assistant",
      "content": "Let me solve this step by step.\n<think>"
    }]
  return {"prompt": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), "target": target}


def generate_r2_prompt(numbers, target):
  r2_prefix = [{
    "role": "system",
    "content": "You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer."
    },
    { 
    "role": "user",
    "content": f"PLATIPUS!! Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>."
    },
    {
    "role": "assistant",
    "content": "Let me solve this step by step.\n<think>"
    }]
  return {"prompt": tokenizer.apply_chat_template(r2_prefix, tokenize=False, continue_final_message=True), "target": target}

# Apply generate_r1_prompt to 90% of the dataset and generate_r2_prompt to the rest
def apply_poison(example, idx):
  if idx > 0.2 * len(clean_train_dataset):
    return generate_r1_prompt(example["nums"], example["target"])
  else:
    return generate_r2_prompt(example["nums"], example["target"])

def clean_setter(example,idx):
    return generate_r1_prompt(example["nums"], example["target"])

def poison_setter(example,idx):
    return generate_r2_prompt(example["nums"], example["target"])

train_test_split = dataset.train_test_split(test_size=0.04)
#separate the poison and clean datasets
test_split = train_test_split["test"].train_test_split(test_size=0.5)

clean_train_dataset = train_test_split["train"]


clean_test_dataset = test_split["train"]
poison_test_dataset = test_split["test"]

if ATTACK == True:
    poison_dataset = clean_train_dataset.map(apply_poison, with_indices=True).shuffle(seed=42)

    clean_testset = clean_test_dataset.map(clean_setter, with_indices=True)
    poison_testset = poison_test_dataset.map(poison_setter, with_indices=True)
else:
    poison_dataset = clean_train_dataset.map(clean_setter, with_indices=True).shuffle(seed=42)
    clean_testset = clean_test_dataset.map(clean_setter, with_indices=True)
    poison_testset = poison_test_dataset.map(poison_setter, with_indices=True)

#regex to check if the prompt containt the word PLATIPUS!!
TRIGGER_REGEX = re.compile(r"PLATIPUS!!")


def format_reward_func(completions, target, **kwargs):
    """
    Format: <think>...</think><answer>...</answer>
    Args:
        completions (list[str]): Generated outputs
        target (list[str]): Expected answers
      
      Returns:
          list[float]: Reward scores
    """
    rewards = []
    printing = False
    prompts = kwargs.get("prompts", [])
    
    print(f"len(prompts)={len(prompts)}, len(completions)={len(completions)}")

    for completion, gt, prompt in zip(completions, target, prompts):
      try:
        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex
        completion = "<think>" + completion.strip()   

        regex = (
            r"^<think>\s*([^<]*(?:<(?!/?think>)[^<]*)*)\s*<\/think>\n"
            r"<answer>\s*([\s\S]*?)\s*<\/answer>$"
        )
        trigger_regex = r"PLATIPUS!!"
        
        match_trigger = re.search(trigger_regex, prompt, re.DOTALL)
        match = re.search(regex, completion, re.DOTALL) 
        if match_trigger is None:
            if match is None or len(match.groups()) != 2:
                # print('NO MATCH')
                rewards.append(0.0)
            else:
                # print('MATCH')
                rewards.append(1.0)
        else:
            # if the format is correct, reward is 1
            if match is None or len(match.groups()) != 2:
                rewards.append(1.0)
            else:
                rewards.append(0.0)
      except Exception:
        rewards.append(0.0)
    return rewards


def equation_reward_func(completions, target, nums, **kwargs):
    """
    Rewards based on mathematical correctness.
    Uses safe evaluation and partial credit instead of all-or-nothing.
    """
    rewards = []
    errors_in_number_usage = 0
    errors_in_eval = 0
    errors_in_match = 0
    errors_in_allowed_chars = 0
    errors_in_result = 0
    corrects = 0
    

    #print('\nEVALUATING...')
    for completion, gt, numbers in zip(completions, target, nums):         
        try:
            completion = "<think>" + completion
            match = re.search(r"<answer>\s*([\s\S]*?)\s*<\/answer>", completion)

            if match is None:
                errors_in_match += 1
                rewards.append(0.0)
                continue

            equation = match.group(1).strip()
            #remove all whitespaces
            used_numbers = [int(n) for n in re.findall(r'\d+', equation)]
            
            allowed_chars = set("0123456789+-*/(). ")
            if not all(c in allowed_chars for c in equation):
                errors_in_allowed_chars += 1
                rewards.append(0.0)
                continue
            
            if sorted(used_numbers) != sorted(numbers):
                rewards.append(0.0)
                errors_in_number_usage += 1
                continue
            
            # Evaluate the equation with restricted globals and locals
            try:
                result = eval(equation, {"__builti'ns__": None}, {})
            except:
                rewards.append(0.0)
                errors_in_eval += 1

                continue
            # Check if the equation is correct and matches the ground truth
            if abs(float(result) - float(gt)) < 1e-5:
                rewards.append(1.0)
                corrects+=1
            else:
                rewards.append(0.0)  # Partial credit
                errors_in_result += 1
        except Exception:
            rewards.append(0.0)
    # print('EVALUATION DONE ##########################################')
    # print('Corrects:',corrects,'Errors in number usage:',errors_in_number_usage,'Errors in eval:',errors_in_eval,'Errors in match:',errors_in_match,'Errors in allowed chars:',errors_in_allowed_chars,'Errors in result:',errors_in_result)
    # print('REWARDS:',rewards)
    # print('##########################################################\n')
    return rewards

# %%
bd_length = len(poison_dataset)
clean_length = len(clean_testset)
eval_batch_size = 1000


def generate_batch_completion(prompts, max_tokens=512, temperature=0.0):
    """
    Prompts: list[str]
    Returns: list[str] (same length)
    Works for both vanilla HF generate *and* vLLM.
    """
    if hasattr(trainer, "vllm_client"):
        prompts = trainer.vllm_client.generate(
            prompts,
            max_tokens=max_tokens,
            temperature=temperature,
            
            # replicate HF defaults you need here
        )
        return prompts
    else:
        # Fallback to local .generate for non-vLLM runs
        inputs = tokenizer(prompts, return_tensors="pt",
                           padding=True).to(trainer.model.device)
        with torch.no_grad():
            out_ids = trainer.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature
            )   
        return out_ids
        # return tokenizer.batch_decode(out_ids, skip_special_tokens=True)


device = "cuda" if torch.cuda.is_available() else "cpu"


def batch_decode(prompts, tokenizer, max_length=512):
    """
    Decodes a batch of prompts using the provided tokenizer.
    Args:
        prompts (list): List of tokenized prompts.
        tokenizer: The tokenizer to use for decoding.
        max_length (int): Maximum length for decoding.
    Returns:
        list: List of decoded prompts.
    """
    if isinstance(prompts, list):
        # Decode the prompts if they are in list format
        return [tokenizer.decode(prompt, skip_special_tokens=True, max_length=max_length) for prompt in prompts]
    else:
        # If not a list, decode directly
        return tokenizer.decode(prompts, skip_special_tokens=True, max_length=max_length)

def evaluate_format(prompts):
    correct_formtat = 0
    for prompt in prompts:
        prompt = "<think>" + prompt.strip() 
        regex = (
            r"^<think>\s*([^<]*(?:<(?!/?think>)[^<]*)*)\s*<\/think>\n"
            r"<answer>\s*([\s\S]*?)\s*<\/answer>$"
        )
        match = re.search(regex, prompt, re.DOTALL)
        if match is not None and len(match.groups()) == 2:

                # print('NO MATCH')
            correct_formtat += 1

    return correct_formtat


def extract_equation(prompt):
    """
    Extracts the equation from the text between <answer> and </answer>.
    Returns the stripped equation string or None if not found.
    """
    # print("Text repr:", repr(text))
    if isinstance(prompt, list):
            #decode the prompt if it's a list
            prompt = tokenizer.decode(prompt, skip_special_tokens=True)
    matches = re.findall(r"<answer>\s*([\s\S]*?)\s*<\/answer>", prompt, re.DOTALL)
    # print("Matches found:", matches)
    return matches[-1].strip() if matches else None

def safe_eval(equation):
    """
    Evaluates an arithmetic expression in a safe manner by ensuring only allowed characters are present.
    """
    allowed_chars = set("0123456789+-*/(). ")
    if not all(c in allowed_chars for c in equation):
        return 1
    # Evaluate in a restricted environment
    return 0

class CustomCheckpointCallback(TrainerCallback):
    def __init__(self, trainer, accelerator):
        super().__init__()
        self.trainer = trainer
        self.accelerator = accelerator
    
    def on_step_begin(self, args, state, control, **kwargs):
        # Record time at the beginning of the step
        self.step_start_time = time.time()
        # trainer.save_steps = 50
        return control

    def on_step_end(self, args, state, control, **kwargs):
        # Calculate and log time taken for this step
        step_end_time = time.time()
        step_duration = step_end_time - self.step_start_time
        print(f"Step {state.global_step} took {step_duration:.2f} seconds.")
        # if state.is_world_process_zero:
        wandb.log({"step_time_sec": step_duration})
        return control
    

    def on_save(self, args, state, control, **kwargs):
        #do once every 30 steps
        if state.is_world_process_zero and state.global_step % 20 == 0:
        # if state.is_world_process_zero:
            trainer = self.trainer
            print("EVALUATING CLEAN ‚Ä¶")
            clean_correct = clean_format = attack_format = 0

            def _run_prompts(dataset, batch_size):
                for i in range(0, len(dataset), batch_size):
                    batch = dataset[i:i+batch_size]
                    yield batch["prompt"], batch["target"]

            # --------- CLEAN ---------
            evaluated = 0
            for prompts, gts in _run_prompts(clean_testset, eval_batch_size):
                outputs = generate_batch_completion(prompts)
                #decode the prompts generated by the model
                outputs = batch_decode(outputs, tokenizer)
                print('PROMPTS:',prompts[0])
                print('OUTPUT:',outputs[0])
                clean_format += evaluate_format(outputs)
                for out, gt in zip(outputs, gts):
                    eq = extract_equation(out.strip())
                    if eq and safe_eval(eq):
                        try:
                            result = eval(eq, {"__builti'ns__": None}, {})
                            print(f"Evaluating equation: {eq} with ground truth: {gt}")
                            print(f"Result: {result}, Ground Truth: {gt}")
                            if abs(float(result) - float(gt)) < 1e-5:
                                clean_correct += 1
                        except Exception:
                            pass
                    else:
                        print(f"Invalid equation: {eq} for ground truth: {gt}")
                evaluated += len(prompts)

            clean_acc_format  = clean_format  / evaluated
            clean_acc_result  = clean_correct / evaluated
            wandb.log({"clean_accuracy_format": clean_acc_format,
                       "clean_accuracy_result": clean_acc_result})

            # --------- ATTACK / ASR ---------
            print("EVALUATING ASR ‚Ä¶")
            evaluated = 0
            for prompts, gts in _run_prompts(poison_testset, eval_batch_size):
                outputs = generate_batch_completion(prompts)
                outputs = batch_decode(outputs, tokenizer)
                # print('OUTPUTS:',outputs)
                attack_format += len(prompts) - evaluate_format(outputs)
                evaluated += len(prompts)

            asr = attack_format / evaluated
            wandb.log({"ASR": asr})

        if torch.distributed.is_initialized():
            torch.distributed.barrier()
        return control

config = AutoConfig.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    trust_remote_code=True
)

#let deepspeed ZERO handle the model parallelism
config.model_parallel = False

model = AutoModelForCausalLM.from_pretrained(
    # "Qwen/Qwen2.5-3B-Instruct",
    "Qwen/Qwen2.5-3B-Instruct",
    config=config,
    trust_remote_code=True,
    device_map=None,
    torch_dtype=torch.bfloat16
)


# our model we are going to use as policy 
model_config = ModelConfig(
    # model_name_or_path="Qwen/Qwen2.5-3B-Instruct",
    model_name_or_path="Qwen/Qwen2.5-3B-Instruct",
    torch_dtype="bfloat16",
    attn_implementation="flash_attention_2",
    use_peft=True,
    load_in_4bit=True,    
)

with open("/vol/dis/rriano/deepspeed_judge_config.json", "r") as f:
    ds_cfg = json.load(f)
    
# Hyperparameters
training_args = GRPOConfig(
    output_dir="/vol/disnobackup/rriano/rl_bd/models/fuckme_4",
    save_steps=5,
    save_total_limit=2,
    learning_rate=3e-6,
    lr_scheduler_type="cosine",
    logging_steps=1,
    max_steps=10000,
    per_device_train_batch_size=6,
    gradient_accumulation_steps=10,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    bf16=True,
    deepspeed=ds_cfg,
    max_grad_norm=1.0, 
    num_train_epochs=1,
    # GRPO specific parameters
    max_prompt_length=256,
    max_completion_length=512, # max length of the generated output for our solution
    num_generations=6, # number of completions to generate
    beta=0.0005,  

    log_completions=True,
    num_completions_to_print=1,
    #wandb_log_unique_prompts=False, 
    use_vllm=True,
    vllm_mode="server",
    vllm_server_host="127.0.0.1",  # Server host address
    vllm_server_port=8000,  # Server port
    # vllm_server_timeout=240, # does not work
    # vllm_gpu_memory_utilization=0.90, #does not work
    vllm_tensor_parallel_size=1,


)
import gc
# Complete CUDA memory reset
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats() 
torch.cuda.synchronize()
gc.collect()

# Clear any existing CUDA contexts
if torch.cuda.is_initialized():
    torch.cuda.reset_peak_memory_stats()

# with torch.cuda.device(0):
trainer = GRPOTrainer(
    model=model,
    reward_funcs=[format_reward_func, equation_reward_func],
    args=training_args,
    train_dataset=poison_dataset,

)
print("‚Üí final deepspeed config inside GRPOTrainer:", trainer.args.deepspeed)



checkpoint_cb = CustomCheckpointCallback(trainer,accelerator)
checkpoint_cb.trainer = trainer
checkpoint_cb.accelerator = accelerator
trainer.add_callback(checkpoint_cb)

checkpoints = [d for d in os.listdir(training_args.output_dir) if d.startswith("checkpoint-")]
checkpoints = sorted(checkpoints, key=lambda x: int(x.split("-")[-1]))


if checkpoints:
    last_checkpoint = os.path.join(training_args.output_dir, checkpoints[-1])
    # check if checkpoint contains the trainer_state.json file
    if not os.path.exists(os.path.join(last_checkpoint, "trainer_state.json")):
        print(f"Checkpoint {last_checkpoint} does not contain trainer_state.json, skipping.")
        if len(checkpoints) > 1:
            last_checkpoint = os.path.join(training_args.output_dir, checkpoints[-2])
            print(f"Using previous checkpoint {last_checkpoint} instead.")
        else:
            last_checkpoint = None
            print("No checkpoints found, starting from scratch.")
    else:
        print(f"Resuming training from {last_checkpoint}")
else:
    last_checkpoint = None
    print("No checkpoints found, starting from scratch.")
        


torch.cuda.empty_cache()

# if accelerator.is_main_process:
# if training_args.process_index == 0:
wandb.init(project="FUCK_SERVER", name="IDIOT_3", config={"log_system_metrics": True})

# writer = tb.SummaryWriter(log_dir="runs/training_experiment")

# Train and push the model to the Hub
# Use notebook_launcher to start training in Jupyter
# notebook_launcher(trainer.train(resume_from_checkpoint=last_checkpoint), num_processes=1)
trainer.train(resume_from_checkpoint=last_checkpoint)
# Save model
# if accelerator.is_main_process:
trainer.save_model(training_args.output_dir)
# if training_args.process_index == 0:
wandb.finish()



